# Deep Learning Specialization

Instructor: [Andrew Ng](http://www.andrewng.org/)
In five courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach. 


## Programming Assignments

## Course 1: Neural Networks and Deep Learning

**Week 1: Introduction to deep learning**
- Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.

**Week 2: Neural Networks Basics**
- Python Basics with Numpy and Logistic Regression with a Neural Network mindset.

**Week 3: Shallow neural networks**
- Understand the key parameters in a neural network's architecture. Planar data classification with a hidden layer

**Week 4: Deep Neural Networks**
- Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision. 


## Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

- This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results.  

**Week 1: Practical aspects of Deep Learning**
- Understand industry best-practices for building deep learning applications. Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking along with implementation.

**Week 2: Optimization algorithms**
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 

**Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks**
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance. 
- Implement a neural network in TensorFlow.


## Course 3: Structuring Machine Learning Projects

  Objectives:  
  + Understand how to diagnose errors in a machine learning system, and 
  + Be able to prioritize the most promising directions for reducing error
  + Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance
  + Know how to apply end-to-end learning, transfer learning, and multi-task learning

**There is no Program Assigments for this course. But this course comes with very interesting case study quizzes**.
  

## Course 4: Convolutional Neural Networks

  Objectives:  
  + Understand how to build a convolutional neural network, including recent variations such as residual networks.
  + Know how to apply convolutional networks to visual detection and recognition tasks.
  + Know to use neural style transfer to generate art.
  + Be able to apply these algorithms to a variety of image, video, and other 2D or 3D data.

**Week 1 - Foundations of Convolutional Neural Networks**
- Build Convolutional Model in python from scratch.

**Week 2 - Deep convolutional models: case studies**
- Build Residual Network in Keras.

**Week 3 - Object detection**
- Learn how to apply your knowledge of CNNs to one of the toughest but hottest field of computer vision: Object detection. Autonomous driving application - Car detection.

**Week 4 - Special applications: Face recognition & Neural style transfer**
- Discover how CNNs can be applied to multiple fields, including art generation and face recognition. 
- Build Face Recognition model for the Happy House. Implement Art Generation with Neural Style Transfer.
  

## Course 5: Sequence Models

  Objectives:
  + Understand how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs.
  + Be able to apply sequence models to natural language problems, including text synthesis. 
  + Be able to apply sequence models to audio applications, including speech recognition and music synthesis.
  
**Week 1 - Recurrent Neural Networks**
- Build a Recurrent Neural Network in python from scratch. Implement Character-Level Language Modeling to generate Dinosaur names. Generate music to Improvise a Jazz Solo with an LSTM Network.

**Week 2 - Natural Language Processing & Word Embeddings**
- Using word vector representations and embedding layers you can train recurrent neural networks with outstanding performances in a wide variety of industries. Examples of applications are sentiment analysis, named entity recognition and machine translation.

**Week 3 - Sequence models & Attention mechanism**
- Sequence models can be augmented using an attention mechanism. This algorithm will help your model understand where it should focus its attention given a sequence of inputs.
- Implement Neural machine translation with attention and Trigger word detection.




# Natural Language Processing Specialization

This Specialization is designed and taught by two experts in NLP, machine learning, and deep learning. Younes Bensouda Mourri is an Instructor of AI at Stanford University who also helped build the Deep Learning Specialization. Łukasz Kaiser is a Staff Research Scientist at Google Brain and the co-author of Tensorflow, the Tensor2Tensor and Trax libraries, and the Transformer paper.


## Programming Assignments

## Course 1: Natural Language Processing with Classification and Vector Spaces

**Week 1: Logistic Regression for Sentiment Analysis of Tweets**
- Classify positive or negative sentiment in tweets using Logistic Regression

**Week 2: Naïve Bayes for Sentiment Analysis of Tweets**
- Classify positive or negative sentiment in tweets using more advanced model

**Week 3: Vector Space Models**
- Vector space models capture semantic meaning and relationships between words. Use them to discover relationships between words, then visualize their relationships in two dimensions using PCA (dimensionality reduction technique).

**Week 4: Word Embeddings and Locality Sensitive Hashing for Machine Translation**
- Write a simple English-to-French translation algorithm using pre-computed word embeddings and locality sensitive hashing to relate words via approximate k-nearest neighbors search


## Course 2: Natural Language Processing with Probabilistic Models

**Week 1: Auto-correct using Minimum Edit Distance**
- Create a simple auto-correct algorithm using minimum edit distance and dynamic programming

**Week 2: Part-of-Speech (POS) Tagging**
- Learn about Markov chains and Hidden Markov models, then use them to create part-of-speech tags for a Wall Street Journal text corpus. Apply the Viterbi algorithm for POS tagging, which is important for computational linguistics

**Week 3: N-gram Language Models**
- Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter (similar models are used for translation, determining the author of a text, and speech recognition)

**Week 4: Word2Vec and Stochastic Gradient Descent**
- Learn about how word embeddings carry the semantic meaning of words, which makes them much more powerful for NLP tasks, then build your own Continuous bag-of-words model that uses a neural network to compute word embeddings from Shakespeare text. 


## Course 3: Natural Language Processing with Sequence Models

**Week 1: Neural Networks for Sentiment Analysis**
- Train a neural network with GLoVe word embeddings to perform sentiment analysis on tweets

**Week 2: Recurrent Neural Networks for Language Modeling**
- Learn about the limitations of traditional language models and see how RNNs and GRUs use sequential data for text prediction. Build your own next-word generator using a simple RNN on Shakespeare text data.

**Week 3: LSTMs and Named Entity Recognition**
- Learn about how long short-term memory units (LSTMs) solve the vanishing gradient problem, and how Named Entity Recognition systems quickly extract important information from text. Build your own Named Entity Recognition system using an LSTM and data from Kaggle.

**Week 4: Siamese Networks**
- Learn about Siamese networks, a special type of neural network made of two identical networks that are eventually merged together. Build your own Siamese network that identifies question duplicates in a dataset from Quora.


## Course 4: Natural Language Processing with Attention Models

**Week 1: Neural Machine Translation with Attention**
- Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism. Build a Neural Machine Translation model with Attention that translates English sentences into German.

**Week 2: Text Summarization with Transformer Models**
- Compare RNNs and other sequential models to the more modern Transformer architecture. Build a transformer model that generates text summaries.

**Week 3: Question-Answering with Transformer Models**
- Explore transfer learning with state-of-the-art models like T5 and BERT, then build a model that can perform question answering task.

**Week 4: Chatbots with a Reformer Model**
- Examine some unique challenges Transformer models face and their solutions, then build a chatbot using a Reformer model.
